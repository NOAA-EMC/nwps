Release Notes:  nwps v1.4.0 - released to NCO on February 11, 2022

Transition from WCOSS Cray to WCOSS2

Where is the release tag on subversion/git/vlab?
https://github.com/NOAA-EMC/nwps.git Tag: IT-nwps.v1.4.0-20220211

List of external software used (anything outside of your vertical structure), 
including compilers and version numbers for everything. Software used must be 
a minimal list of modules/versions specified per job:

jnwps_ofs_prep.ecf/jnwps_rtofs_prep.ecf/jnwps_estofs_prep.ecf/jnwps_psurge_prep.ecf:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-mpich/8.1.7
cray-pals/1.0.12
cfp/2.0.4
wgrib2/2.0.8
python/3.8.6
iobuf/2.0.10

jnwps_prep.ecf:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-pals/1.0.12
wgrib2/2.0.8

jnwps_forecast_cg1.ecf:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-pals/1.0.12
cray-mpich/8.1.7
wgrib2/2.0.8
python/3.8.6
jasper/2.0.25
libpng/1.6.37
zlib/1.2.11
w3nco/2.4.1
cfp/2.0.4
iobuf/2.0.10

jnwps_post_cg1.ecf:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-pals/1.0.12
wgrib2/2.0.8
cfp/2.0.4
cpe/21.09
proj/7.1.0
geos/3.8.1
libjpeg/9c
python/3.8.6

jnwps_prdgen_cg1.ecf:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-pals/1.0.12
wgrib2/2.0.8
libjpeg/9c
grib_util/1.2.2
util_shared/1.4.0

jnwps_wavetrack_cg1.ecf:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-pals/1.0.12
wgrib2/2.0.8
cfp/2.0.4
cpe/21.09
proj/7.1.0
geos/3.8.1
libjpeg/9c
python/3.8.6
iobuf/2.0.10

jnwps_prdgen_cg0.ecf:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-pals/1.0.12
wgrib2/2.0.8
libjpeg/9c
grib_util/1.2.2
util_shared/1.4.0

jnwps_forecast_cgn.ecf:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-pals/1.0.12
wgrib2/2.0.8
python/3.8.6
jasper/2.0.25
libpng/1.6.37
zlib/1.2.11
w3nco/2.4.1
cfp/2.0.4

jnwps_post_cgn.ecf:
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-pals/1.0.12
wgrib2/2.0.8
cfp/2.0.4
cpe/21.09
proj/7.1.0
geos/3.8.1
libjpeg/9c
python/3.8.6

jnwps_prdgen_cgn.ecf
envvar/1.0
prod_envir/2.0.5
prod_util/2.0.8
PrgEnv-intel/8.1.0
craype/2.7.8
intel/19.1.3.304
cray-pals/1.0.12
wgrib2/2.0.8
libjpeg/9c
grib_util/1.2.2
util_shared/1.4.0

List of all code/scripts modified with this release (relative to v1.3.9)
NOTE: Renamed files are indicated with an ->

versions/build.ver
versions/run.ver

modulefiles/build_nwps.modules
Removed: modulefiles/NWPS/v1.3.0
Removed: sorc/degrib-2.15.cd/build_psurge.module

sorc/make_NWPS.sh
sorc/get_externals.sh

sorc/make_degrib-2.15.sh
sorc/degrib-2.15.cd/build.sh
sorc/degrib-2.15.cd/Makefile
sorc/emapf-c/Makefile
sorc/libaat/Makefile
sorc/make_psurge2nwps.sh
sorc/psurge2nwps.cd/makefile

sorc/swan.fd/platform.pl
sorc/estofs_padcirc.fd/work/cmplrflags.mk
sorc/estofs_padcirc.fd/work/actualflags.txt
sorc/estofs_padcirc.fd/work/makefile
sorc/punswan4110.fd/platform.pl
sorc/punswan4110.fd/macros.inc
sorc/punswan4110.fd/swanparll.ftn
sorc/make_nwps_utils.sh

ecf/nwps/jnwps_ofs_prep.ecf
ecf/nwps/jnwps_rtofs_prep.ecf
ecf/nwps/jnwps_estofs_prep.ecf
ecf/nwps/jnwps_psurge_prep.ecf
ecf/nwps/jnwps_datachk.ecf
ecf/nwps/jnwps_prep.ecf
ecf/nwps/jnwps_forecast_cg1.ecf
ecf/nwps/jnwps_forecast_cgn.ecf
ecf/nwps/jnwps_post_cg1.ecf 
ecf/nwps/jnwps_post_cgn.ecf 
ecf/nwps/jnwps_prdgen_cg0.ecf
ecf/nwps/jnwps_prdgen_cg1.ecf
ecf/nwps/jnwps_prdgen_cgn.ecf
ecf/nwps/jnwps_wavetrack_cg1.ecf

jobs/JNWPS_DATACHK
jobs/JNWPS_DATACHK.saveOrig
jobs/JNWPS_DATACHK.turnoff_rzdm
jobs/JNWPS_OFS_PREP
jobs/JNWPS_PREP
jobs/JNWPS_FORECAST
jobs/JNWPS_POST
jobs/JNWPS_PRDGEN
jobs/JNWPS_WAVETRACK

scripts/exnwps_ofs_prep.sh.ecf  ->  scripts/exnwps_ofs_prep.sh
scripts/exnwps_datachk.sh.ecf  ->  scripts/exnwps_datachk.sh
scripts/exnwps_prep.sh.ecf  ->  scripts/exnwps_prep.sh
scripts/exnwps_forecast.sh.ecf  ->  scripts/exnwps_forecast.sh
scripts/exnwps_post.sh.ecf  ->  scripts/exnwps_post.sh
scripts/exnwps_prdgen.sh.ecf  ->  scripts/exnwps_prdgen.sh
scripts/exnwps_wavetrack.sh.ecf  ->  scripts/exnwps_wavetrack.sh

ush/pm/GraphicOutput.pm 
ush/pm/SetupCG.pm
ush/make_psurge_init.sh 
ush/make_psurge.sh
ush/swanexe.sh
ush/waveTracking.pl
ush/ww3_systrackexe.sh
ush/run_posproc_cgncmdfile.sh
ush/run_posproc.sh

ush/python/cur.py
ush/python/erosion.py
ush/python/htsgw.py
ush/python/owash.py
ush/python/period.py
ush/python/rip.py
ush/python/spc1d.py
ush/python/swell.py 
ush/python/wind.py
ush/python/wlev.py
ush/python/plot_nwps_run_from_grib2.sh
ush/python/shiproute_cur.py
ush/python/shiproute.py
ush/python/ww3_systrk_cluster_parallel.py
ush/python/partition.py

Changed to developer files (not relevant to production):

dev/ecf/jnwps_ofs_prep.ecf
dev/ecf/jnwps_datachk.ecf
dev/ecf/jnwps_prep.ecf.tmpl
dev/ecf/jnwps_forecast_cg1.ecf.tmpl
dev/ecf/jnwps_forecast_cgn.ecf.tmpl
dev/ecf/jnwps_post_cg1.ecf.tmpl
dev/ecf/jnwps_post_cgn.ecf.tmpl
dev/ecf/jnwps_prdgen_cg0.ecf.tmpl
dev/ecf/jnwps_prdgen_cg1.ecf.tmpl
dev/ecf/jnwps_prdgen_cgn.ecf.tmpl
dev/ecf/jnwps_wavetrack_cg1.ecf.tmpl

dev/jobs/JNWPS_DATACHK
dev/jobs/JNWPS_DATACHK_RET
dev/jobs/JNWPS_FORECAST 
dev/jobs/JNWPS_FORECAST_RET 
dev/jobs/JNWPS_OFS_PREP
dev/jobs/JNWPS_POST
dev/jobs/JNWPS_POST_RET
dev/jobs/JNWPS_PRDGEN
dev/jobs/JNWPS_PRDGEN_RET
dev/jobs/JNWPS_PREP
dev/jobs/JNWPS_PREP_RET 
dev/jobs/JNWPS_WAVETRAC
dev/jobs/JNWPS_WAVETRACK_RET

dev/scripts/exnwps_ofs_prep.sh.ecf  ->  dev/scripts/exnwps_ofs_prep.sh
dev/scripts/exnwps_datachk.sh.ecf  ->  dev/scripts/exnwps_datachk.sh
dev/scripts/exnwps_prep.sh.ecf  ->  dev/scripts/exnwps_prep.sh
dev/scripts/exnwps_forecast.sh.ecf  ->  dev/scripts/exnwps_forecast.sh
dev/scripts/exnwps_post.sh.ecf  ->  dev/scripts/exnwps_post.sh
dev/scripts/exnwps_prdgen.sh.ecf  ->  dev/scripts/exnwps_prdgen.sh
dev/scripts/exnwps_wavetrack.sh.ecf  ->  dev/scripts/exnwps_wavetrack.sh
dev/scripts/exnwps_datachk_ret.sh.ecf  ->  dev/scripts/exnwps_datachk_ret.sh
dev/scripts/exnwps_prep_ret.sh.ecf  ->  dev/scripts/exnwps_prep_ret.sh


What changes were made to the above code/scripts to support the new architecture?

1. The NWPS codebase requires HDF5 v1.8.9 and NetCDF v4.2. Since these are not available in the WCOSS2 system modules, local builds of these have been added. They install under lib/.

2. Various makefiles under sorc/ have been updated for the new WCOSS2 architecture and local HDF5/NetCDF builds (see above). 

3. Code profiling by GDIT showed a memory bottleneck due to numerous calls to file INQUIRE by the dynamic code SWAN. These calls were reduced in the file:
sorc/punswan4110.fd/swanparll.ftn

4. Perl modules updated to work with updated library version.

5. Updated execution commands in ush scripts from aprun to mpiexec.

6. Updated ush/swanexe.sh with new core/node configuration on WCOSS2.

7. Python plotting scripts: Removed unnecessary matplotlib.use('Agg',warn=False).

8. Scripts in ecf/nwps/*.ecf updated from #BSUB to #PBS

9. Scripts in jobs/ updated for new COMOUT and NWGES paths.

10. ex-scripts in scripts/ renamed to remove .ecf.

NOTE: Please check the following NCO transfer scripts for new COM and NWGES paths (these could not be tested):

parm/transfer_nwps.list
parm/transfer_nwps_nwges.list
parm/transfer_nwps_nwges_main.list
parm/transfer_nwps_nwges_main.list.tmp
parm/transfer_nwps_nwges_hotstart1.list
parm/transfer_nwps_nwges_hotstart2.list
parm/transfer_nwps_nwges_hotstart3.list
parm/transfer_nwps_nwges_hotstart4.list
parm/transfer_nwps_nwges_histlog.list
parm/transfer_nwps_regions.list
parm/transfer_nwps_ofs.list


Were any other changes made that arenâ€™t directly related to the transition?
No

Are there any changes to incoming data needs or outgoing products? 
No

If output filenames are changing, list the current and new filename
N/A

Compute resource information, for every job:
*** Providing PBS and/or ecFlow submission scripts as part of release tag is preferred; 
if they are provided then resource information is not needed in the release notes.
See ecf/nwps/ directory. NOTE: For one of the 36 model domains, the resources in jnwps_forecast_cg1.ecf need to be increased as follows:

#PBS -l select=1:ncpus=120  ->  #PBS -l select=2:ncpus=72


Runtime changes compared to current production (/gpfs/dell1/nco/ops/com/logs/runtime/daily/* for current stats)

OFS_PREP:
                   WCOSS2    WCOSS1 prod
OFS_PREP estofs    60 min    48 min
OFS_PREP rtofs     32 min    11 min
OFS_PREP psurge    24 min    32 min

WFO domains:
       WCOSS2    WCOSS1 prod
bro:   20 min    20 min 
crp:   24 min    24 min
hgx:   68 min    32 min
lch:   32 min    26 min
lix:   50 min    43 min
mob:   56 min    32 min
tae:   61 min    41 min
tbw:   55 min    36 min
key:  110 min    76 min
mfl:  162 min    84 min
sju:   92 min    78 min
mlb:   72 min    56 min
jax:   47 min    44 min
chs:  133 min    42 min
ilm:   54 min    39 min
mhx:   87 min    67 min
akq:  166 min    60 min
lwx:   18 min    17 min
phi:   84 min    35 min
okx:   81 min    61 min
box:  120 min    66 min
gyx:   61 min    42 min
car:   56 min    36 min
sgx:  117 min    43 min
lox:   81 min    46 min
mtr:  123 min    64 min
eka:   29 min    33 min
mfr:   60 min    55 min
pqr:   42 min    39 min
sew:   33 min    30 min
hfo:   77 min    71 min
gum:   45 min    36 min
ajk:   92 min    86 min
aer:   93 min    94 min
alu:   88 min    71 min
afg:   47 min    67 min


Disk space required per day or per cycle; data retention on disk will remain the same unless otherwise requested.
Unchanged

Dissemination will remain the same unless otherwise communicated to NCO and a PNS/SCN issued
Unchanged.

HPSS archive retention will remain the same unless approval granted by HPCRAC
Unchanged.

What are your up and downstream dependencies?
Unchanged.
Upstream: GFE forecaster wind fields from DCOM.
          GFS v16.2
          ESTOFS v2.1
           Seaice_analysis v4.5
          RTOFS v2.2
          PSURGE v2.10
Downstream: None.

Provide a pointer to your COMOUT directory that was used during testing: 

/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/v1.4/
ofs.20220125              <- Output of jnwps_estofs_prep and jnwps_rtofs_prep
sr.20220125
er.20220125
wr.20220125
ar.20220125
pr.20220125
ofs.20180912_test_psurge  <- Output of jnwps_psurge_prep test (H Florence)
sr.20220125_test_gfs      <- Output of GFS failover test

Canned data from WCOSS1 Cray for comparison:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/prod/
ofs.20220125
sr.20220125
er.20220125
wr.20220125
ar.20220125
pr.20220125
ofs.20180912_test_psurge


IMPLEMENTATION INSTRUCTIONS

I) Checking out the code from VLab

Make the home directory for the model in $NWROOT:
> mkdir nwps
> cd nwps

Clone the NWPS code from the following Github repo, and checkout the release tag:
> git clone https://github.com/NOAA-EMC/nwps.git Tag: IT-nwps.v1.4.0-20220211
> git checkout IT-nwps.v1.4.0-20220211

II) Building the executables

First define the path variable ${NWPSdir} in your profile file, which points to the base of the code checked out under (I) above.
> export NWPSdir=$(pwd)

Next, change directory to ${NWPSdir}/sorc/, and execute the general NWPS install script. This single step will install the total package, including all libraries and binary compilations:

> cd ${NWPSdir}/sorc
> ./make_NWPS.sh

Once the compilations are done, all executables are moved to ${NWPSdir}/exec, and the system will be ready.


TEST INSTRUCTIONS:

1. Run jnwps_estofs_prep from canned data for 2022/01/25

In jobs/JNWPS_OFS_PREP, point COMINestofs to the canned input data here:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/estofs/prod/estofs.20220125

Point COMINsicem1 and COMINsice to respectively:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/omb/prod/sice.20220124
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/omb/prod/sice.20220125

Set PDY=20220125, execute the job, and compare to results under:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/v1.4/ofs.20220125/estofs
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/prod/ofs.20220125/estofs

2. Run jnwps_rtofs_prep from canned data for 2022/01/25

In jobs/JNWPS_OFS_PREP, point COMINrtofs to the canned input data in this path:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/rtofs/prod/rtofs.20220125

Set PDY=20220125, execute the job, and compare to results under:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/v1.4/ofs.20220125/rtofs
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/prod/ofs.20220125/rtofs

3. Run jnwps_purge_prep from canned data for 2018/09/12

Since PSurge is run on demand, there was no recent input or output available on WCOSS1. However, MDL made canned input data from H FLorence (2018) available. This can be used to run jnwps_purge_prep, and verify against the COMOUT results obtained during testing. Note also that the PSurge results are blended onto the output water levels from jnwps_estofs_prep. This output has been obtained from WCOSS1, and is used here.

In jobs/JNWPS_OFS_PREP, point COMINpsurge to the canned input data in this path:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/psurge.v2.10/2018-Florence-Adv54-2.10.1/com/psurge/v2.10/psurge.20180912/

Copy the following WCOSS1 canned *output* data from jnwps_estofs_prep to the working directory $DATA:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/prod/ofs.20180912_test_psurge/estofs

Set PDY=20180912, execute jnwps_purge_prep, and compare to results under:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/v1.4/ofs.20180912_test_psurge/psurge

4. Test the NWPS on-demand runs (36 domains) for 2022/01/25

The NWPS on-demand runs use the following jnwps_ofs_prep output in $COMOUT prepared under 1 and 2 above:
ofs.20220125/estofs
ofs.20220125/rtofs

In addition, in jobs/JNWPS_PREP, point COMINgfs and COMINwave to this canned data:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/gfs/prod/gfs.20220125

Also, copy the following hotstart (restart) files to $NWGES:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/nwges/hotstart

Note that these hotstart files cannot be older than 3 days, otherwise NWPS will reject them. Therefore, before running the model, perform:

touch hotstart/???/??/20220125.??00*
touch hotstart/???/??/PE????/20220125.??00
touch hotstart/???/??/PE????/20220126.??00

Finally, in jobs/JNWPS_PREP, point dcom to the canned data here:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/dcom/prod/20220125/wgrbbul/nwps

Next, set PDY=20220125, and execute the suite of on-demand NWPS runs for all of its 36 domains. This will use the GFE forecaster wind fields from the canned dcom directory above. Note that NWPS will only run the newest of the input cycles available from each WFO, so expect the model to run only once for each of the 36 domains.

Compare the results of this test to the WCOSS2 test output here:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/v1.4/
sr.20220125
er.20220125
wr.20220125
ar.20220125
pr.20220125

Also compare against the WCOSS1 prod output below. Note that these directories contain the output for *all* of the cycles on 20220125. Compare the above results against the last cycle for each WFO only:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/prod/
sr.20220125
er.20220125
wr.20220125
ar.20220125
pr.20220125

5. Test the NWPS on-demand runs in GFS fail-over mode for 2022/01/25 (1 domain)

When there is an error in the GFE forecaster wind fields, NWPS is configured to fail over to GFS wind fields. This did not occur on 2022/01/25, but has been simulated by removing the GFE forcing file from the dcom tarball for WFO TBW. To execute this test, set dcom to:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/dcom/prod/20220125/wgrbbul/nwps_test_gfs

Now execute the on-demand run for the TBW domain as done in 4 above. Compare the output to the WCOSS2 test data here (no WCOSS1 prod output available for comparison):
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/v1.4/sr.20220125_test_gfs

Note that the output below should be displayed in the log file:
/lfs/h2/emc/couple/noscrub/Andre.VanderWesthuysen/nwps/com/nwps/v1.4/sr.20220125_test_gfs/tbw/18/Warn_Forecaster_TBW.20220125.txt

NWPS run started on Tue Feb  8 17:07:40 UTC 2022
WARNING: We received bad GFE wind file, forecaster wind file has bad values. Will fail over to GFS data.
Starting model run with gfs winds
Run settings: RUNLEN=144 WNA=WNAWave NESTS=Yes RTOFS=No WINDS=GFS WEB=Yes PLOT=Yes USERDELTAC=600 HOTSTART=TRUE WATERLEVELS=ESTOFS MODELCORE=UNSWAN
Forecast analysis time: 20220125 18Z

